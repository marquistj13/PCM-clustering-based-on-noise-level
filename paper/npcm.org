#+STARTUP: content
#+OPTIONS: 
#+OPTIONS: toc:nil
# set DATE to void to avoid it's display
#+DATE: 
#+LATEX_CLASS: IEEEtran
#+LaTeX_CLASS_OPTIONS: [journal]

#+LATEX_HEADER: \usepackage[thmmarks, amsmath, thref]{ntheorem}
#+LATEX_HEADER: \theoremstyle{definition}
# Adds automatic line break, if heading is too long
#+LATEX_HEADER: \makeatletter \renewtheoremstyle{plain} {\item{\theorem@headerfont ##1\ ##2\theorem@separator}~}  {\item{\theorem@headerfont ##1\ ##2\ (##3)\theorem@separator}~}
#+LATEX_HEADER: \theoremheaderfont{\normalfont\bfseries}
#+LATEX_HEADER: \theoremseparator{:}
#+LATEX_HEADER: \theorembodyfont{\normalfont}
#+LATEX_HEADER: \theoremsymbol{\ensuremath{\blacksquare}}
# The Theorem
#+LATEX_HEADER: \newtheorem{definition}{Definition}
# The Proof
#+LATEX_HEADER: \newtheorem*{proof}{Proof}
# the Proposition
#+LATEX_HEADER: \newtheorem{prop}{Proposition}

# multi figures
#+LATEX_HEADER: \usepackage[caption=false,font=footnotesize]{subfig}

# The algorithm
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \renewcommand{\algorithmicrequire}{\textbf{Input:}}
#+LATEX_HEADER: \newcommand{\crhd}{\raisebox{.25ex}{$\rhd$}}
#+LATEX_HEADER: \renewcommand{\algorithmiccomment}[1]{{\hspace{-0.6cm}$\crhd$ {\it {#1}}}}


# In IEEEtran_HOWTO the equations section on page 8. this 2500 config is to estore IEEEtran ability to automatically break within multiline equations
#+LATEX_HEADER: \interdisplaylinepenalty=2500

#+TITLE: PCM Clustering based on Noise Level
#+BEGIN_LaTeX
\begin{abstract}
Possibilistic c-Means (PCM) based clustering algorithms  are widely used in the literature. The unified framework (UPCM) of  PCM and adaptive PCM (APCM) controls the clustering process from the perspective of uncertainty and UPCM can effectively discover the underlying structure of the dataset. However, UPCM has three parameters to choose.
In this paper, we present an extension of UPCM, i.e., noise level based pcm (NPCM), to ease the parameter-choosing issue and also to  improve UPCM.
NPCM runs with two parameters, $m_{ini}$ is the potentially over-specified initial number of clusters, $\alpha$ is the noise level of the dataset which characterizes minimum closeness of clusters. NPCM then discovers the true number of clusters. $\alpha$ controls the closeness of generated clusters. Another property of NPCM is that the bandwidth (radius) of each cluster can be correctly estimated, furthermore, NPCM automatically calculates the uncertainty of the estimated bandwidth ($\sigma_v$). More specifically, a large bandwidth corresponds to a large bandwidth uncertainty $\sigma_v$. 
\end{abstract}
#+END_LaTeX 
** Introduction
Our contributions are summarized as follows:
1. We find that both APCM and UPCM suffer from the problem of background noise clusters, i.e., the background noise cluster are highly possible to become very large and finally merge with other physical clusters, so that there is only one big cluster in the final result.
** Background Knowledge
*** The PCM and APCM Clustering Algorithms
The objective of Possibilistic c-means (PCM) \cite{krishnapuram_possibilistic_1993} is to minimize the following cost:
#+BEGIN_LaTeX
\begin{equation}
J(\mathbf{\Theta},\mathbf{U})=\sum_{j=1}^{c}J_j=\sum_{j=1}^{c}\left[\sum_{i=1}^{N}u_{ij}d_{ij}^2+\gamma_j \sum_{i=1}^{N}f(u_{ij})\right]
\end{equation}
#+END_LaTeX
where $f(\cdot)$ can be chosen as:
#+BEGIN_LaTeX
\begin{equation}
f(u_{ij})=u_{ij}\log u_{ij}-u_{ij}
\end{equation}
#+END_LaTeX 
$\mathbf{\Theta}=(\boldsymbol{\theta}_1,\ldots,\boldsymbol{\theta}_c)$ is a $c$-tuple of prototypes, $d_{ij}$ is the distance of feature point $\mathbf{x}_i$ to prototype $\boldsymbol{\theta}_j$, $N$ is the total number of feature vectors, $c$ is the number of clusters, and $\mathbf{U}=[u_{ij}]$ is a $N\times c$ matrix where $u_{ij}$ denotes the /degree of compatibility/ of $\mathbf{x}_i$ to the $j\text{th}$ cluster $C_j$ which is represented by $\boldsymbol{\theta}_j$. $\gamma_j$ can be seen as a bandwidth parameter of the possibility (membership) distribution for each cluster and is usually fixed in PCM based algorithms. Note that either $\gamma_j$ or $\sqrt{\gamma_j}$ can be referred to as the bandwidth for convenience in this paper.

Compared with Fuzzy c-means (FCM) \cite{bezdek_pattern_2013}, PCM relaxes the constraint that the memberships of a datum to all clusters sum to $1$. So the generated memberships can be indicated as the typicality of a point to the cluster. This modification also leads to higher noise immunity with respect to FCM based algorithms \cite{barni_comments_1996}.

Minimizing $J(\mathbf{\Theta},\mathbf{U})$ with respect to $u_{ij}$ and $\boldsymbol{\theta}_j$ leads to the the following two update equations:
#+BEGIN_LaTeX
\begin{IEEEeqnarray}{ll}
u_{ij}&=\exp\left(-\frac{d^2_{ij}}{\gamma_j}\right) \label{pcm_u_update}  \\
\boldsymbol{\theta}_j&=\frac{\Sigma_{i=1}^Nu_{ij}\mathbf{x}_i}{\Sigma_{i=1}^Nu_{ij}} \label{pcm_theta_update}
\end{IEEEeqnarray}
#+END_LaTeX

The major problem of PCM is that its performance relies heavily on good initial partitions and parameters \cite{nasraoui_improved_1996}. More specifically, the $c$ dense regions found may be coincident, as reported in \cite{barni_comments_1996}. Adaptive PCM (APCM) \cite{xenaki_novel_2016} solves this problem by adapting $\gamma_j$ at each iteration, and the clusters with $\gamma_j=0$ are eliminated. To handle the case where two physical clusters with very different variance are located very close to each other, APCM introduces a parameter $\alpha$ to manually scale the bandwidth:
#+BEGIN_LaTeX
\begin{equation}
\label{corrected_eta}
\gamma_j=\frac{\hat{\eta}}{\alpha}\eta_j
\end{equation}
#+END_LaTeX 
where $\hat{\eta}$ is a constant defined as the minimum among all initial $\eta_j\text{s}$, $\hat{\eta}=\min_j\eta_j$, and $\alpha$ is chosen so that the quantity $\hat{\eta}/\alpha$ equals to the mean absolute deviation ($\eta_j$)  of the smallest physical cluster formed in the dataset.
$\eta_j$ is updated at each iteration as the /mean absolute deviation/ of the most compatible to cluster $C_j$ data points which form a set $A_j$, i.e., $A_j=\{\mathbf{x}_i|u_{ij}=\max_r u_{ir}\}$.
#+BEGIN_LaTeX
\begin{equation}
\label{apcm_eta_update}
\eta_j=\frac{1}{n_j}\sum_{\mathbf{x}_i\in A_j}||\mathbf{x}_i-\boldsymbol{\mu}_j||
\end{equation}
#+END_LaTeX 
where $n_j$ and $\boldsymbol{\mu}_j$ are the number of points in $A_j$ and the mean vector of points in $A_j$ respectively.
*** Conditional Fuzzy Set and the UPCM Algorithm
The unified framework (UPCM) of PCM and APCM controls the clustering process from the perspective of uncertainty and UPCM can effectively discover the underlying structure of the dataset. However, UPCM has three parameters to choose.
** Our Algorithm
In this section, we first present motivations of proposed noise-level based PCM clustering algorithm (NPCM), which is developed based on UPCM. To address the issue of background noise clusters, we propose to eliminate low-density clusters in the initial partition. Then, we cancel out the parameter $\sigma_v$ in UPCM by utilizing the interplay between $\alpha$ and $\sigma_v$. Finally, we analyze the benefit and the problem after adopting the adaptive $\sigma_v$ approach in NPCM, and summarize the algorithm in Algorithm \ref{alg:npcm}.
*** Motivations
1. The performance of clustering algorithm should be robust to background noise. We find that both APCM and UPCM suffer from the problem of background noise clusters, i.e., the background noise cluster are highly possible to become very large and finally merge with other physical clusters, so that there is only one large cluster in the final clustering result.
2. The clustering algorithm should have as less parameters as possible, and the parameters should be intuitive to choose. APCM exerts strong control over the bandwidth correction process, i.e., the estimated bandwidth is directly scaled by the user-specified parameter $\alpha$ which is difficult to choose basing on intuition. In order to handle the uncertainty of the estimated bandwidth in a more fuzzy way, UPCM introduces an uncertainty parameter $\sigma_v$ to regulate the shape of the membership function. However, choosing $\sigma_v$ depends on the noise-level parameter $\alpha$. The experiments in \cite{hou_pcm_2016} suggest that a small $\alpha$ should correspond to a small $\sigma_v$, and a large $\alpha$ to a large $\sigma_v$. So we can exploit this intuition to cancel out the parameter $\sigma_v$.
*** Initialization in NPCM
There are two issues with the initialization of NPCM. First, as in APCM and UPCM, NPCM needs an over-specified number of clusters $m_{ini}$ of the true number of clusters $m$. In the initial partition of the dataset, there should be at least one cluster placed near each physical cluster. 
It's shown in \cite{panda_comparing_2012} that K-Means is faster than FCM. So we choose K-Means to get the initial partitions of the dataset. Let $A_j^{ini}$ be the set of points $\mathbf{x}_i$ that belong to cluster $C_j$ and $n_j$ be the size of $A_j^{ini}$. Then the we set
#+BEGIN_LaTeX
\begin{IEEEeqnarray}{ll}
\boldsymbol{\theta}_j &= \frac{\Sigma_{i}\mathbf{x}_i}{n_j}  \quad \text{for}\;\mathbf{x}_i \in A_j^{ini} \label{npcm_ini_theta}\\
\eta_j &= \frac{1}{n_j}\sum_{\mathbf{x}_i \in A_j^{ini}}||\mathbf{x}_i-\boldsymbol{\theta}_j|| \label{npcm_ini_eta}
\end{IEEEeqnarray}
#+END_LaTeX 
Second, as stated in the motivations, the background noise clusters in the initial partition should be eliminated. To address this issue, we define the density of a cluster as:
#+BEGIN_LaTeX
\begin{equation}
\label{npcm_density}
\rho_j=\frac{n_j}{\eta_j^d}
\end{equation}
#+END_LaTeX
where $d$ is the dimension of $\mathbf{x}_i$. Let $\rho_0=\max_j\rho_j$. Then cluster $C_j$ is a noise cluster and is eliminated if $\rho_j<0.1\rho_0$.
*** Modeling the relation between $\alpha$ and $\sigma_v$
In UPCM, the noise-level parameter $\alpha$ is introduced to reduce the influence of points in other clusters $\boldsymbol{\theta}_{i\neq j}$ on the $\boldsymbol{\theta}_j$ update. Specifically, $\alpha$ measures the closeness of two cluster prototypes in the clustering result. When a large $\alpha$ is specified, we consider that there may be clusters very close to each other, and the $\eta_j$ estimated in this case may be very uncertain. With the above interpretation of $\alpha$, the interplay between $\alpha$ and $\sigma_v$ becomes simple and intuitive, i.e., a large specification of noise level $\alpha$ means that fewer points (that we consider as good points) are actually contributed to the adaption of prototype $\boldsymbol{\theta}_j$, so we should specify a large $\sigma_v$ to give the clusters in one physical cluster more mobility to merge; on the other hand, a small specification of $\alpha$ means that we are less uncertain about the estimated bandwidth, and more points are contributed to the adaption of $\boldsymbol{\theta}_j$, so we should also specify a small $\sigma_v$ \cite{hou_pcm_2016}. 

Before proceeding, we review the the role of $\sigma_v$ in the clustering process of UPCM, which offers us insights into fuzzy clustering. UPCM exploits the conditional fuzzy set to incorporate the uncertainty of the estimated bandwidth. As can be seen from , the shape of the membership function becomes flatter when $\sigma_v$ or $d_{ij}$ increases. Note that a larger bandwidth corresponds a flatter membership curve. This observation suggests that the bandwidth itself can indicate the uncertainty of the estimated bandwidth, i.e., a large estimated bandwidth should correspond to a large $\sigma_v$. We will see that the formulation of NPCM meets this requirement.

From , we can calculate the distance $d_{j\alpha}$ beyond which a point can't be used to contribute to the adaption of cluster $C_j$ by letting
#+BEGIN_LaTeX
\begin{equation}
\exp\left(-\frac{(d_{j\alpha})^2}{\gamma_j}\right)=\alpha,
\end{equation}
#+END_LaTeX
which leads to
#+BEGIN_LaTeX
\begin{equation}
\label{npcm_d_alpha}
d_{j\alpha}=\sqrt{-\ln\alpha}\left(\eta_j+\sqrt{-\ln\alpha}\sigma_v\right)
\end{equation}
#+END_LaTeX

When there is no uncertainty in the estimated bandwidth, we get $d_{j\alpha}^0=\sqrt{-\ln\alpha}\eta_j$. For a fixed $\alpha$, a large $\sigma_v$ will cause $d_{j\alpha}$ to become larger, which reduces the effect of $\alpha$. This observation, together with the intuitive interplay between $\alpha$ and $\sigma_v$ we get from UPCM, suggests that we can explicitly specify a relation between $\alpha$ and $\sigma_v$. More specifically, we define the effect of $\sigma_v$ as the correction of $d_{j\alpha}^0$ by considering the uncertainty of the estimated bandwidth:
#+BEGIN_LaTeX
\begin{equation}
\frac{d_{j\alpha}-d_{j\alpha}^0}{d_{j\alpha}^0}=\frac{\sqrt{-\ln\alpha}\sigma_v}{\eta_j}=0.2,
\end{equation}
#+END_LaTeX
which leads to 
#+BEGIN_LaTeX
\begin{equation}
\label{npcm_sig_alpha_relation}
\sigma_v=0.2\frac{\eta_j}{\sqrt{-\ln\alpha}}
\end{equation}
#+END_LaTeX
Note that we can choose a value that is not 0.2 in the above formulation. From \eqref{npcm_sig_alpha_relation}, we can see that the cluster with large $\eta_j$ has a large bandwidth estimation uncertainty $\sigma_v$. Then update of the membership function is modified as follows:
#+BEGIN_LaTeX
\begin{equation}
\label{npcm_u_update}
\mu_{ij}=\exp\left(-\frac{d_{ij}^2}{\gamma_j}\right)
\end{equation}
#+END_LaTeX
where $\gamma_j=\left(0.5\eta_{j}+0.5\sqrt{\eta_{j}^{2}+0.8d_{ij}\eta_j/\sqrt{-\ln\alpha}}\right)^2$ and $d_{ij}=||\mathbf{x}_i-\boldsymbol{\theta}_j||$.
*** The NPCM algorithm
The proof of cluster elimination and convergence of the prototypes to the center of dense regions is given in the Appendix. When convergence is reached, there is only one cluster in each physical cluster. However, it is seen from \eqref{npcm_sig_alpha_relation} that for cluster $C_j$, the adaption of $\eta_j$ is a positive feedback process, i.e., a large $\eta_j$ leads to a large $\sigma_v$ which means that there may be more points involved to calculate $\eta_j$ in the next iteration, so $\eta_j$ may become larger. The benefit of this fact is that the generated $\eta_j$ can automatically increase to fit the physical cluster. But there can also be situations where cluster $C_j$ becomes unexpectedly large because boundary points between $C_j$ and other clusters gradually become points of $C_j$ due to the positive feedback process, and as a result, $\eta_k$ of the nearby smaller cluster $C_k$ is under-estimated. Note that UPCM does not have this problem because all clusters in UPCM have the same $\sigma_v$. To solve this problem, we modify the $\eta_j$ update as:
#+BEGIN_LaTeX
\begin{equation}
\label{npcm_eta_update}
\eta_j=\frac{1}{n_j}\sum_{\mathbf{x}_i\in A_j}||\mathbf{x}_i-\boldsymbol{\theta}_j|| \quad \text{for}\;u_{ij} \geq 0.01.
\end{equation}
#+END_LaTeX
where $A_j$ and $n_j$ have the same meaning as in \eqref{apcm_eta_update}. The rationale is that, the update process of $\eta_j$ should not be too sensitive to the point $\mathbf{x}_i$ near the boundary of clusters, and by so doing, the iteration times may also be reduced.

From the previous analysis, the NPCM algorithm can be summarized in Algorithm \ref{alg:npcm}.
#+BEGIN_LaTeX
\begin{algorithm}[H]
\caption{ [$\Theta$, $U$, $label$] = NPCM($X$, $m_{ini}$, $\alpha$)}
\label{alg:npcm}
\begin{algorithmic}[1]
\Require {$X$, $m_{ini}$, $\alpha$}
\State $m=m_{ini}$
\State \textbf{Set:} $\alpha=10^{-5}$ if $\alpha==0$
\State \textbf{Set:} $\alpha=1-10^{-5}$ if $\alpha==1$
\Statex {\Comment {Initialization and possible noise cluster elimination}}
\State Run K-Means.
\State Initialize $\theta_j$ and $\eta_j$ via \eqref{npcm_ini_theta} and \eqref{npcm_ini_eta}
\State Caculate $\rho_j$ via \eqref{npcm_density}
\State \textbf{Set:} $\rho_0=\max_j\rho_j$
\State Cluster $j$ is eliminated if $\rho_j<0.1\rho_0$
\State \textbf{Set:} $m=m-p$ if $p$ clusters are eliminated
\Repeat
\State Update $U$ via \eqref{npcm_u_update}
\State Update $\Theta$ via \eqref{upcm_theta_update}
\Statex {\Comment {Possible cluster elimination}}
\For{$i \leftarrow 1 \textbf{ to } N$}
\State \textbf{Set:} $label(i)=r$ if $u_{ir}=\max_j u_{ij}$
\EndFor
\State Cluster $j$ is eliminated if $j \notin label$
\State \textbf{Set:} $m=m-p$ if  $p$ clusters are eliminated
\Statex {\Comment {Bandwidth update and possible cluster elimination}}
\State Update $\eta_j$ via \eqref{npcm_eta_update}
\State Cluster $j$ is eliminated if $\eta_j=0$ (This happens if there is only one point in Cluster $j$)
\State \textbf{Set:} $m=m-p$ if  $p$ clusters are eliminated
\Until{the change in $\theta_j$'s between two successive iterations becomes sufficiently small}\\
\Return {$\Theta$, $U$, $label$}
\end{algorithmic}
\end{algorithm}
#+END_LaTeX
*** ApTEMPT
\appendix
In this appendix, we prove the cluster elimination and the convergence of the prototypes to the center of dense regions. Because some convergence results of APCM \cite{xenaki_novel_2016} are applicable to NPCM, we only give the essential part of the proof. 

We consider the continuous case where data points are modeled by a random variable $\mathbf{x}$ that follows a continuous pdf distribution $p(\mathbf{x})$. In this case, the update equations are given below:
#+BEGIN_LaTeX
\begin{equation}
\boldsymbol{\theta}_j^{t+1}=\frac{\int_{R_j^t} u_{j}^t(\mathbf{x})\mathbf{x}p(\mathbf{x})d\mathbf{x}}{\int_{R_j^t} u_{j}^t(\mathbf{x})p(\mathbf{x})d\mathbf{x}} 
\end{equation}
#+END_LaTeX
where $R_j^t=\{\mathbf{x}|u_{j}^t(\mathbf{x}) \geq \alpha\}$,
#+BEGIN_LaTeX
\begin{IEEEeqnarray}{ll}
u_{j}^t(\mathbf{x}) &= \exp\left(\frac{||\mathbf{x}-\boldsymbol{\theta}_j^t||^2}{\gamma_j^t}\right) \\
\gamma_j^t &= \left(0.5\eta_{j}+0.5\sqrt{\eta_{j}^{2}+0.8d_{j}\eta_j/\sqrt{-\ln\alpha}}\right)^2
\end{IEEEeqnarray}
#+END_LaTeX
and 
#+BEGIN_LaTeX
\begin{equation}
\eta_{j} = \frac{\int_{T_j^{t}} ||\mathbf{x}-\boldsymbol{\theta}_j^{t}||p(\mathbf{x})d\mathbf{x}}{\int_{T_j^{t}} p(\mathbf{x})d\mathbf{x}}
\end{equation}
#+END_LaTeX
with $T_j^{t}=\{\mathbf{x}|u_{j}^{t}(\mathbf{x})=\max_r u_{r}^{t}(\mathbf{x}), u_{j}^t(\mathbf{x}) \geq 0.01\}$.

The above equations define the iterative scheme $\boldsymbol{\theta}_j^{t+1}=f(\boldsymbol{\theta}_j^{t})$, where
#+BEGIN_LaTeX
\begin{equation}
\label{npcm_iteration_scheme}
f(\boldsymbol{\theta}_j^t)=\frac{\int_{R_j^t} \exp\left(-\frac{\|\mathbf{x}-\boldsymbol{\theta}_j^t\|^2}{\gamma_j^t}\right)\mathbf{x}p(\mathbf{x})d\mathbf{x}}{\int_{R_j^t} \exp\left(-\frac{\|\mathbf{x}-\boldsymbol{\theta}_j^t\|^2}{\gamma_j^t}\right)p(\mathbf{x})d\mathbf{x}} 
\end{equation}
#+END_LaTeX


#+BEGIN_LaTeX
\begin{prop}
Assume that $p(\mathbf{x})$ is a zero mean normal distribution ${\cal N}(\mathbf{0},\sigma^2I)$. Then the center $\mathbf{c}=\mathbf{0}$ of $p(\mathbf{x})$ is a fixed point for the iterative scheme defined by \eqref{npcm_iteration_scheme}. Furthermore, the fixed point $\mathbf{0}$ of the scheme $\boldsymbol{\theta}^{t+1}=f(\boldsymbol{\theta}^{t})$ is stable.
\label{prop_fix_stable}
\end{prop}

\begin{proof}
See the proof of Proposition 3 and Proposition 4 in \cite{xenaki_novel_2016}.
\end{proof}
#+END_LaTeX

In the general case where data form more than one dense regions, Proposition \ref{prop_fix_stable} is still valid, assuming that a proper $\alpha$ is specified so that the influence on a prototype that belongs to a given dense region from points from other dense regions is negligible.

#+BEGIN_LaTeX
\begin{prop}
Let $\boldsymbol{\theta}_1$, $\boldsymbol{\theta}_2$ be two cluster prototypes with $\eta_1>\eta_2$ in the same dense region. Then cluster $C_2$ represented by $\boldsymbol{\theta}_2$ will be eliminated.
\label{prop_eliminate}
\end{prop}

\begin{proof}
 We first calculate the geometrical locus of the points $\mathbf{x}$ having $u_2(\mathbf{x})>u_1(\mathbf{x})$, where $u_j(\mathbf{x})=\exp\left(-\frac{d_j^2(\mathbf{x})}{\gamma_j}\right)$ and $d_j(\mathbf{x})=\|\mathbf{x} - \boldsymbol{\theta}_j\|2$, $j=1,2$.
From \eqref{npcm_d_alpha} and \eqref{npcm_sig_alpha_relation}, we get $d_{u_1}=\sqrt{-\ln u_1}\left(\eta_1+\sqrt{-\ln u_1}\sigma_{v_1}\right)=1.2\sqrt{-\ln u_1}\eta_1$ and $d_{u_2}=1.2\sqrt{-\ln u_2}\eta_2$, where we use $u_1$ and $u_2$ to represent $u_1(\mathbf{x})$ and $u_2(\mathbf{x})$ respectively. 

For the points $\mathbf{x}$ that meet $u_1<u_2$, we have 
\begin{equation*}
\frac{d_{u_1}}{d_{u_2}}=\frac{1.2\sqrt{-\ln u_1}\eta_1}{1.2\sqrt{-\ln u_2}\eta_2}>\frac{\eta_1}{\eta_2}(1+\epsilon)=k'>\frac{\eta_1}{\eta_2}=k>1
\end{equation*}
where $\epsilon\in(0,+\infty)$. Then we get $\|\mathbf{x} - \boldsymbol{\theta}_1\|^2 > k'\|\mathbf{x} - \boldsymbol{\theta}_2\|^2$, and we have after some algebra
\begin{equation}
\label{hypersphere}
\|\mathbf{x}-\frac{k'\boldsymbol{\theta}_2-\boldsymbol{\theta}_1}{k'-1}\|^2 = \frac{k'}{(k'-1)^2}\|\boldsymbol{\theta}_2-\boldsymbol{\theta}_1\|^2\equiv r^2
\end{equation}

Utilizing Proposition \ref{prop_fix_stable}, we have that $\boldsymbol{\theta}_1$ and $\boldsymbol{\theta}_2$ converge towards $\mathbf{c}$. Thus, the distance between them decreases towards zero, i.e., 
\begin{equation}
\|\boldsymbol{\theta}_1(t)-\boldsymbol{\theta}_2(t)\|\rightarrow 0
\label{eqprop51}
\end{equation}		

So the radius $r$ in \eqref{hypersphere} tends to zero as $t$ increases, which means that there will be no points in Cluster $C_2$ and $C_2$ will be eliminated.

Note that $k'$ is larger than the $k$ used in the proof of APCM, so convergence of NPCM is faster than APCM. 
See the proof of Proposition 2 and Proposition 5 in \cite{xenaki_novel_2016} for details.
\end{proof}
#+END_LaTeX




#+BEGIN_LaTeX
\bibliographystyle{IEEEtran}
\bibliography{D:/emacs/etc/ZoteroOutput,IEEEabrv}
#+END_LaTeX
